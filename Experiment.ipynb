{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.transforms import Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from typing import Callable, List, Tuple, Union\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import copy\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torch.utils.data as data\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "\n",
    "def bbox_process(bbox):\n",
    "    x_min = int(bbox[0])\n",
    "    y_min = int(bbox[1])\n",
    "    x_max = x_min + int(bbox[2])\n",
    "    y_max = y_min + int(bbox[3])\n",
    "    return list(map(int, [x_min, y_min, x_max, y_max]))\n",
    "\n",
    "\n",
    "def get_tensor(normalize=True, toTensor=True):\n",
    "    transform_list = []\n",
    "    if toTensor:\n",
    "        transform_list += [torchvision.transforms.ToTensor()]\n",
    "\n",
    "    if normalize:\n",
    "        transform_list += [torchvision.transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                                (0.5, 0.5, 0.5))]\n",
    "    return torchvision.transforms.Compose(transform_list)\n",
    "\n",
    "def get_tensor_clip(normalize=True, toTensor=True):\n",
    "    transform_list = []\n",
    "    if toTensor:\n",
    "        transform_list += [torchvision.transforms.ToTensor()]\n",
    "\n",
    "    if normalize:\n",
    "        transform_list += [torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073),\n",
    "                                                (0.26862954, 0.26130258, 0.27577711))]\n",
    "    return torchvision.transforms.Compose(transform_list)\n",
    "\n",
    "def get_image_coords(bbox_corners, lidar2image):\n",
    "    \"\"\"\n",
    "    Get the camera coordinates of the 3D bounding box\n",
    "\n",
    "    Args:\n",
    "        bbox_corners: np.array, shape (8, 3)\n",
    "        lidar2image: np.array, shape (4, 4)\n",
    "\n",
    "    Returns:\n",
    "        np.array, shape (8, 2)\n",
    "        Each row is the x, y coordinates of the 3D bounding box in the image\n",
    "        x \\in [0, W], y \\in [0, H]\n",
    "    \"\"\"\n",
    "    coords = np.concatenate(\n",
    "        [bbox_corners.reshape(-1, 3), np.ones((8, 1))], axis=-1\n",
    "    )\n",
    "    lidar2image = lidar2image.copy().reshape(4, 4)\n",
    "    coords = coords @ lidar2image.T\n",
    "    coords = coords.reshape(8, 4)\n",
    "\n",
    "    coords[..., 2] = np.clip(coords[..., 2], a_min=1e-5, a_max=1e5)\n",
    "    coords[..., :2] /= coords[..., 2, None]\n",
    "\n",
    "    coords = coords[..., :2].reshape(8, 2)\n",
    "\n",
    "    return coords\n",
    "\n",
    "def rotate_bbox(bbox_corners, angle=0):\n",
    "    \"\"\"\n",
    "    Rotate the 3D bounding box around its y-axis\n",
    "\n",
    "    Args:\n",
    "        bbox_corners: np.array, shape (8, 3)\n",
    "        angle: float, rotation angle in degrees\n",
    "\n",
    "    Returns:\n",
    "        np.array, shape (8, 3)\n",
    "        Each row is the x, y, z coordinates\n",
    "    \"\"\"\n",
    "    if angle == 0:\n",
    "        return bbox_corners\n",
    "    \n",
    "    bbox_corners = copy.deepcopy(bbox_corners)\n",
    "    angle = np.deg2rad(angle)\n",
    "    center = np.mean(bbox_corners, axis=0)\n",
    "    bbox_corners -= center\n",
    "\n",
    "    rotation_matrix = np.array([\n",
    "        [np.cos(angle),-np.sin(angle), 0],\n",
    "        [np.sin(angle), np.cos(angle), 0],\n",
    "        [0, 0, 1],\n",
    "    ])\n",
    "    bbox_corners = bbox_corners @ rotation_matrix.T\n",
    "\n",
    "    bbox_corners += center\n",
    "    return bbox_corners\n",
    "\n",
    "def translate_bbox(bbox_corners, new_center):\n",
    "    \"\"\"\n",
    "    Translate the 3D bounding box to a new center\n",
    "\n",
    "    Args:\n",
    "        bbox_corners: np.array, shape (8, 3)\n",
    "        new_center: np.array, shape (3,)\n",
    "\n",
    "    Returns:\n",
    "        np.array, shape (8, 3)\n",
    "        Each row is the x, y, z coordinates\n",
    "    \"\"\"\n",
    "    bbox_corners = copy.deepcopy(bbox_corners)\n",
    "    center = np.mean(bbox_corners, axis=0)\n",
    "    bbox_corners -= center\n",
    "    bbox_corners += new_center\n",
    "    return bbox_corners\n",
    "\n",
    "def get_camera_coords(bbox_corners, lidar2camera):\n",
    "    \"\"\"\n",
    "    Get the camera coordinates of the 3D bounding box\n",
    "\n",
    "    Args:\n",
    "        bbox_corners: np.array, shape (8, 3)\n",
    "        lidar2camera: np.array, shape (4, 4)\n",
    "\n",
    "    Returns:\n",
    "        np.array, shape (8, 3)\n",
    "        Each row is the x, y, z coordinates of the 3D bounding box in the camera frame\n",
    "    \"\"\"\n",
    "    coords = np.concatenate(\n",
    "        [bbox_corners.reshape(-1, 3), np.ones((8, 1))], axis=-1\n",
    "    )\n",
    "    lidar2camera = lidar2camera.copy().reshape(4, 4)\n",
    "    coords = coords @ lidar2camera.T\n",
    "    coords = coords.reshape(8, 4)\n",
    "\n",
    "    return coords[..., :3]\n",
    "\n",
    "def get_inpaint_mask(bbox_corners, transform, H, W, expand_ratio=0.1):\n",
    "    bbox_corners = expand_bbox_corners(bbox_corners, expand_ratio)\n",
    "    mask = np.zeros((H, W), dtype=np.uint8)\n",
    "\n",
    "    coords = get_image_coords(bbox_corners, transform)\n",
    "\n",
    "    # Draw 3D boxes\n",
    "    for polygon in [\n",
    "        [0, 1, 2, 3],\n",
    "        [4, 5, 6, 7],\n",
    "        [0, 1, 5, 4],\n",
    "        [2, 3, 7, 6],\n",
    "        [0, 4, 7, 3],\n",
    "        [1, 5, 6, 2],\n",
    "    ]:\n",
    "        points = coords[polygon].astype(np.int32)\n",
    "        cv2.fillPoly(mask, [points], 1, cv2.LINE_AA)\n",
    "\n",
    "    mask = ((mask > 0.5) * 255).astype(np.uint8)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def draw_projected_bbox(image, bbox_coords, color=(0, 165, 255), thickness=2):\n",
    "    \"\"\"\n",
    "    Draw projected 3D bounding box on the image\n",
    "\n",
    "    Args:\n",
    "        image: np.array, shape (H, W, 3)\n",
    "        bbox_coords: np.array, shape (8, 2)\n",
    "        color: tuple, color of the bbox\n",
    "        thickness: int, thickness of the lines\n",
    "\n",
    "    Returns:\n",
    "        np.array, shape (H, W, 3)\n",
    "    \"\"\"\n",
    "    H, W = image.shape[:2]\n",
    "    bbox_coords = bbox_coords.copy()\n",
    "    bbox_coords[..., 0] *= W\n",
    "    bbox_coords[..., 1] *= H\n",
    "\n",
    "    canvas = image.copy()\n",
    "\n",
    "    for start, end in [\n",
    "        (0, 1), (0, 3), (3, 2), (1, 2), # bottom lines\n",
    "        (1, 5), (0, 4), (3, 7), (2, 6), # vertical lines\n",
    "        (4, 7), (4, 5), (5, 6), (6, 7), # top lines\n",
    "    ]:\n",
    "        cv2.line(\n",
    "            canvas,\n",
    "            bbox_coords[start].astype(np.int32),\n",
    "            bbox_coords[end].astype(np.int32),\n",
    "            color,\n",
    "            thickness,\n",
    "            cv2.LINE_AA,\n",
    "        )\n",
    "\n",
    "    # Draw arrow towards the face 0 1 4 5\n",
    "    center = np.mean(bbox_coords, axis=0).astype(int)\n",
    "    tip = np.mean(bbox_coords[[0, 1, 4, 5]], axis=0).astype(int)\n",
    "    cv2.arrowedLine(\n",
    "        canvas,\n",
    "        center,\n",
    "        tip,\n",
    "        color,\n",
    "        thickness,\n",
    "        cv2.LINE_AA,\n",
    "        tipLength=0.1,\n",
    "    )\n",
    "\n",
    "    return canvas\n",
    "\n",
    "\n",
    "def get_2d_bbox(bbox_corners, transform, H, W, expand_ratio=0.1):\n",
    "    bbox_corners = expand_bbox_corners(bbox_corners, expand_ratio)\n",
    "    coords = get_image_coords(bbox_corners, transform)\n",
    "\n",
    "    minxy = np.min(coords, axis=-2)\n",
    "    maxxy = np.max(coords, axis=-2)\n",
    "\n",
    "    bbox_2d = np.concatenate([minxy, maxxy], axis=-1).astype(int)\n",
    "    bbox_2d[0::2] = np.clip(bbox_2d[0::2], a_min=0, a_max=W - 1)\n",
    "    bbox_2d[1::2] = np.clip(bbox_2d[1::2], a_min=0, a_max=H - 1)\n",
    "\n",
    "    return bbox_2d\n",
    "\n",
    "\n",
    "def expand_bbox_corners(bbox_corners, expand_ratio=0.1):\n",
    "    if expand_ratio == 0:\n",
    "        return bbox_corners\n",
    "\n",
    "    bbox_corners = copy.deepcopy(bbox_corners)\n",
    "    center = np.mean(bbox_corners, axis=0)\n",
    "    bbox_corners -= center\n",
    "    bbox_corners *= (1 + expand_ratio)\n",
    "    bbox_corners += center\n",
    "\n",
    "    return bbox_corners\n",
    "\n",
    "\n",
    "class NuScenesDataset(data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        state,\n",
    "        object_database_path,\n",
    "        scene_database_path,\n",
    "        object_classes,\n",
    "        expand_mask_ratio=0,\n",
    "        expand_ref_ratio=0,\n",
    "        ref_aug=True,\n",
    "        ref_mode=\"same-ref\", # same-ref, track-ref, random-ref, no-ref\n",
    "        image_height=512,\n",
    "        image_width=512,\n",
    "        reference_image_min_h=40,\n",
    "        reference_image_min_w=40,\n",
    "        frustum_iou_max=0.7,\n",
    "        camera_visibility_min=0.5,\n",
    "        normalize_bbox=True,\n",
    "        rot_every_angle=0,\n",
    "        specific_scene=None, # used for rotation test\n",
    "    ) -> None:\n",
    "        self.state = state\n",
    "        self.ref_aug = ref_aug\n",
    "        self.ref_mode = ref_mode\n",
    "        self.expand_mask_ratio = expand_mask_ratio\n",
    "        self.expand_ref_ratio = expand_ref_ratio\n",
    "        self.normalize_bbox = normalize_bbox\n",
    "        self.specific_scene = specific_scene\n",
    "\n",
    "        self.all_objects_meta = pd.read_csv(object_database_path, index_col=0)\n",
    "        # filter out small, occluded objects\n",
    "        self.all_objects_meta = self.all_objects_meta[\n",
    "            (self.all_objects_meta[\"reference_image_h\"] >= reference_image_min_w) &\n",
    "            (self.all_objects_meta[\"reference_image_w\"] >= reference_image_min_h) &\n",
    "            (self.all_objects_meta[\"max_iou_overlap\"] <= frustum_iou_max) &\n",
    "            self.all_objects_meta[\"object_class\"].isin(object_classes) &\n",
    "            (self.all_objects_meta[\"camera_visibility_mask\"] >= camera_visibility_min)\n",
    "        ]\n",
    "\n",
    "        if self.state == \"test\":\n",
    "            # select an object from each class\n",
    "            self.objects_meta = self.all_objects_meta.groupby(\"object_class\").apply(\n",
    "                lambda x: x.sample(1)\n",
    "            ).reset_index(drop=True)\n",
    "        else:\n",
    "            self.objects_meta = self.all_objects_meta\n",
    "\n",
    "        if rot_every_angle != 0:\n",
    "            angles = np.arange(0, 360, rot_every_angle)\n",
    "            self.objects_meta = pd.concat(\n",
    "                [self.objects_meta] * len(angles), ignore_index=True\n",
    "            )\n",
    "            self.objects_meta[\"bbox_rot_angle\"] = np.tile(angles, len(self.objects_meta) // len(angles))\n",
    "\n",
    "        with open(scene_database_path, \"rb\") as f:\n",
    "            self.scenes_info = pickle.load(f)\n",
    "\n",
    "        # Image transforms\n",
    "        ref_augs = [\n",
    "            A.Resize(height=224, width=224)\n",
    "        ]\n",
    "        if ref_aug:\n",
    "            ref_augs += [\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.Rotate(limit=20),\n",
    "                A.Blur(p=0.3),\n",
    "                A.ElasticTransform(p=0.3)\n",
    "            ]\n",
    "        self.ref_transform = A.Compose(ref_augs)\n",
    "        self.resize = T.Resize([image_height, image_width])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        object_meta = self.objects_meta.iloc[index]\n",
    "\n",
    "        if self.specific_scene is not None:\n",
    "            scene_info = self.scenes_info[self.specific_scene]\n",
    "            # always use the front camera when specific_scene is provided\n",
    "            cam_idx = 0\n",
    "        else:\n",
    "            scene_info = self.scenes_info[object_meta[\"scene_token\"]]\n",
    "            cam_idx = object_meta[\"cam_idx\"]\n",
    "\n",
    "        id_name = self.get_id_name(object_meta)\n",
    "        \n",
    "        lidar2image = scene_info[\"lidar2image_transforms\"][cam_idx]\n",
    "        lidar2camera = scene_info[\"lidar2camera_transforms\"][cam_idx]\n",
    "        image_path = scene_info[\"image_paths\"][cam_idx]\n",
    "        bbox_3d = scene_info[\"gt_bboxes_3d_corners\"][object_meta[\"scene_obj_idx\"]]\n",
    "\n",
    "        # Image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        W, H = image.size\n",
    "        image_tensor = get_tensor()(np.array(image))\n",
    "        image_tensor = self.resize(image_tensor)\n",
    "\n",
    "        # Reference\n",
    "        ref_image, ref_label = self.get_reference(object_meta)\n",
    "\n",
    "        ref_image = self.ref_transform(image=ref_image)[\"image\"]\n",
    "        ref_image = Image.fromarray(ref_image)\n",
    "        ref_image_tensor = get_tensor_clip()(ref_image)\n",
    "\n",
    "        bbox_rot_angle = object_meta.get(\"bbox_rot_angle\", 0)\n",
    "        id_name += \"_rot-{}\".format(bbox_rot_angle)\n",
    "        bbox_3d = rotate_bbox(bbox_3d, bbox_rot_angle)\n",
    "\n",
    "        if self.specific_scene is not None:\n",
    "            bbox_3d = translate_bbox(bbox_3d, [0, 9, -1])\n",
    "       \n",
    "        bbox_image_coords = get_image_coords(bbox_3d, lidar2image)\n",
    "        if self.normalize_bbox:\n",
    "            bbox_image_coords[..., 0] /= W\n",
    "            bbox_image_coords[..., 1] /= H\n",
    "        bbox_camera_coords = get_camera_coords(bbox_3d, lidar2camera)\n",
    "\n",
    "        # Mask\n",
    "        mask_np = get_inpaint_mask(\n",
    "            bbox_3d, lidar2image, H, W, self.expand_mask_ratio\n",
    "        )\n",
    "        mask_image = Image.fromarray(mask_np)\n",
    "        mask_tensor = 1 - get_tensor(normalize=False, toTensor=True)(mask_image)\n",
    "        mask_tensor = (self.resize(mask_tensor) > 0.5).float()\n",
    "\n",
    "        # Inpainted image\n",
    "        inpaint_tensor = image_tensor * mask_tensor\n",
    "\n",
    "        data = {\n",
    "            \"id_name\": id_name,\n",
    "            \"GT\": image_tensor,\n",
    "            \"inpaint_image\": inpaint_tensor,\n",
    "            \"inpaint_mask\": mask_tensor,\n",
    "            \"bbox_image_coords\": bbox_image_coords,\n",
    "            \"cond\": {\n",
    "                \"ref_image\": ref_image_tensor,\n",
    "                \"ref_bbox\": bbox_camera_coords,\n",
    "                \"ref_label\": ref_label,\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.objects_meta)\n",
    "    \n",
    "    def get_reference(self, current_object_meta):\n",
    "        if self.ref_mode == \"no-ref\":\n",
    "            return np.zeros((224, 224, 3), dtype=np.uint8), 0\n",
    "        elif self.ref_mode == \"same-ref\":\n",
    "            reference_meta = current_object_meta\n",
    "        elif self.ref_mode == \"random-ref\":\n",
    "            reference_meta = self.all_objects_meta[\n",
    "                self.all_objects_meta[\"object_class\"] == current_object_meta[\"object_class\"]\n",
    "            ].sample(1).iloc[0]\n",
    "        elif self.ref_mode == \"track-ref\":\n",
    "            reference_meta = self.all_objects_meta[\n",
    "                self.all_objects_meta[\"track_id\"] == current_object_meta[\"track_id\"]\n",
    "            ].sample(1).iloc[0]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid ref_mode\")\n",
    "\n",
    "        ref_obj_idx = reference_meta[\"scene_obj_idx\"]\n",
    "        cam_idx = reference_meta[\"cam_idx\"]\n",
    "        ref_scene_info = self.scenes_info[reference_meta[\"scene_token\"]]\n",
    "        lidar2image = ref_scene_info[\"lidar2image_transforms\"][cam_idx]\n",
    "        image_path = ref_scene_info[\"image_paths\"][cam_idx]\n",
    "\n",
    "        ref_bbox_3d = ref_scene_info[\"gt_bboxes_3d_corners\"][ref_obj_idx]\n",
    "        ref_label = ref_scene_info[\"gt_labels\"][ref_obj_idx]\n",
    "        \n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        W, H = image.size\n",
    "        image_np = np.array(image)\n",
    "\n",
    "        bbox_2d = get_2d_bbox(\n",
    "            ref_bbox_3d, lidar2image, H, W, self.expand_ref_ratio\n",
    "        )\n",
    "        x1, y1, x2, y2 = bbox_2d\n",
    "        w = np.maximum(x2 - x1 + 1, 1)\n",
    "        h = np.maximum(y2 - y1 + 1, 1)\n",
    "        ref_image = image_np[y1:y1+h, x1:x1+w]\n",
    "\n",
    "        return ref_image, ref_label\n",
    "    \n",
    "    def get_id_name(self, object_meta):\n",
    "        id_name = \"sample-{}_track-{}_time-{}_{}_{}\".format(\n",
    "            object_meta[\"scene_token\"],\n",
    "            object_meta[\"track_id\"],\n",
    "            object_meta[\"timestamp\"],\n",
    "            object_meta[\"object_class\"],\n",
    "            self.ref_mode\n",
    "        )\n",
    "        if self.ref_aug:\n",
    "            id_name += \"-aug\"\n",
    "\n",
    "        return id_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed everything\n",
    "seed = 3\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "dataset = NuScenesDataset(\n",
    "    state=\"test\",\n",
    "    object_database_path=\"/mnt/data/mobi/mobi/data/nuscenes/nuscenes_dbinfos_pbe_val.csv\",\n",
    "    scene_database_path=\"/mnt/data/mobi/mobi/data/nuscenes/nuscenes_scene_infos_pbe_val.pkl\",\n",
    "    reference_image_min_h=400,\n",
    "    reference_image_min_w=400,\n",
    "    object_classes=[\"car\"],\n",
    "    rot_every_angle=30,\n",
    "    specific_scene='06be0e3b665c44fa8d17d9f4770bdf9c',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(12):\n",
    "    sample = dataset[i]\n",
    "    segment_id_batch = sample[\"id_name\"]\n",
    "    image_tensor = sample[\"GT\"]\n",
    "\n",
    "    def un_norm(x):\n",
    "        return (Resize([450, 800])(x)+1.0)/2.0\n",
    "\n",
    "    bbox_image_coords = sample['bbox_image_coords']\n",
    "\n",
    "    GT_img = un_norm(image_tensor).cpu().numpy().transpose(1, 2, 0)\n",
    "    GT_img = (GT_img * 255).astype(np.uint8)[..., ::-1]\n",
    "    GT_img = draw_projected_bbox(GT_img, bbox_image_coords)\n",
    "    GT_img = GT_img[..., ::-1]\n",
    "\n",
    "    plt.imshow(GT_img)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find scene given picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = \"n015-2018-10-02-10-50-40+0800__CAM_FRONT__1538448761512460.jpg\"\n",
    "with open(\"data/nuscenes/nuscenes_scene_infos_pbe_val.pkl\", \"rb\") as f:\n",
    "    scenes_info = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scene_token, scene_info in scenes_info.items():\n",
    "    print(scene_info['image_paths'][0])\n",
    "    for image_path in scene_info['image_paths']:\n",
    "        if image in image_path:\n",
    "            print(scene_token)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "image_paths = os.listdir(\"/mnt/data/mobi/mobi/results_test_rotate/exp/results\")\n",
    "# sort\n",
    "image_paths = sorted(image_paths, key=lambda x: int(x.strip('.png').split('-')[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images\n",
    "images = []\n",
    "for image_path in image_paths:\n",
    "    img = cv2.imread(os.path.join(\"/mnt/data/mobi/mobi/results_test_rotate/exp/results\", image_path))\n",
    "    images.append(img)\n",
    "\n",
    "# create mp4 video\n",
    "out = cv2.VideoWriter('output.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 1, (800, 450))\n",
    "for i in range(len(images)):\n",
    "    out.write(images[i])\n",
    "out.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "from scripts.inference import load_model_from_config\n",
    "from ldm.util import instantiate_from_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_config(config, ckpt, verbose=False):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    if \"global_step\" in pl_sd:\n",
    "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    if len(m) > 0 and verbose:\n",
    "        print(\"missing keys:\")\n",
    "        print(m)\n",
    "    if len(u) > 0 and verbose:\n",
    "        print(\"unexpected keys:\")\n",
    "        print(u)\n",
    "\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = OmegaConf.load(\"configs/nusc.yaml\")\n",
    "model = load_model_from_config(config, \"checkpoints/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = torch.load(\"checkpoints/model.ckpt\", map_location=\"cpu\")['state_dict']\n",
    "model2 = torch.load(\"models/Paint-by-Example/2024-03-21T21-08-02_nusc/checkpoints/last.ckpt\", map_location=\"cpu\")['state_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1['learnable_vector'].to(torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in model1.keys():\n",
    "    if 'cond_stage_model' in k:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in model1.keys():\n",
    "    if k not in model2.keys():\n",
    "        print(f\"{k} not in model2\")\n",
    "    elif not torch.equal(model1[k], model2[k]):\n",
    "        print(f\"{k} is not equal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ldm.lr_scheduler import LambdaLinearScheduler, LambdaWarmUpCosineScheduler2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = LambdaLinearScheduler(\n",
    "    warm_up_steps=[0],\n",
    "    f_start=[1e-3],\n",
    "    cycle_lengths=[50000],\n",
    "    f_max=[1],\n",
    "    f_min=[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list = [scheduler.schedule(i) for i in range(50000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(lr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
